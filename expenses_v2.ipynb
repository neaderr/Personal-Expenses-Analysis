{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Personal Expenses Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Read data from CSV**  \n",
    "Export data from the smartphone app I use to collect my expense data. The data comes in a handy CSV format, so I can easily load it into a pandas DataFrame by specifying a delimiter. ~~I also specify other parameters: which columns to load and parsing dates from the 'date' column.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fname = \"data/report_2022-10-16_090806.csv\"\n",
    "# load the data\n",
    "df = pd.read_csv(\n",
    "    fname,\n",
    "    sep=\";\",\n",
    "    usecols=[\n",
    "        \"account\",\n",
    "        \"category\",\n",
    "        \"currency\",\n",
    "        \"amount\",\n",
    "        \"ref_currency_amount\",\n",
    "        \"type\",\n",
    "        \"payment_type\",\n",
    "        \"payment_type_local\",\n",
    "        \"note\",\n",
    "        \"date\",\n",
    "        \"labels\"\n",
    "    ],\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Cleaning and Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Check for duplicated and missing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check non-null count and dtype of each variable\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note and labels variables contain missing values. These fields are optional when I create entries in the app and will not impact the accuracy of the analysis. I will fill empty values with \"NA\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN values with \"NA\"\n",
    "df.fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicated rows and remove if any. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all duplicated values\n",
    "df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is actually one duplicated entry, so I will remove one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicated rows\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transform, add additional variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable \"Category\" actually contains subcategory entries. I will add an additional variable that contains category values.  \n",
    "I will combine this step with adding the nature of the expense -- [need, want].  \n",
    "~~In this step, I add the category names. The exported data set doesn't contain this data, so I copied it manually from the application and created a dictionary(***category : subcategory***)\n",
    "After that, I will map the category value to each row based on the subcategory using pandas **map()**.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distinct subcategories\n",
    "subcategories = df['category'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict with categories as key and subcategory as value and nature [need, want]\n",
    "\n",
    "d = {\n",
    "    \"Food and Drinks\": [\n",
    "        \"Food & Drinks\",\n",
    "        \"Bar, cafe\",\n",
    "        \"Groceries\",\n",
    "        \"Restaurant, fast-food\",\n",
    "        \"Fitness Supplements\",\n",
    "        \"Coffee\",\n",
    "        \"Eating out\"\n",
    "    ],\n",
    "    \"Shopping\": [\n",
    "        \"Shopping\",\n",
    "        \"Clothes & shoes\",\n",
    "        \"Drug-store, chemist\",\n",
    "        \"Electronics, accessories\",\n",
    "        \"Camera expenses\",\n",
    "        \"Free time\",\n",
    "        \"Gifts, joy\",\n",
    "        \"Health and beauty\",\n",
    "        \"Teeth care\",\n",
    "        \"Skincare face\",\n",
    "        \"Supplements\",\n",
    "        \"Medicine\",\n",
    "        \"Home, garden\",\n",
    "        \"Jewels, accessories\",\n",
    "        \"Stationery, tools\",\n",
    "    ],\n",
    "    \"Housing\": [\"Housing\", \"Energy, utilities\", \"Maintenance, repairs\", \"Rent\"],\n",
    "    \"Transportation\": [\n",
    "        \"Transportation\",\n",
    "        \"Business trips\",\n",
    "        \"Long distance\",\n",
    "        \"Public transport\",\n",
    "        \"Taxi\",\n",
    "    ],\n",
    "    \"Vehicle\": [\n",
    "        \"Vehicle\",\n",
    "        \"Fuel\",\n",
    "        \"Leasing\",\n",
    "        \"Parking\",\n",
    "        \"Rentals\",\n",
    "        \"Vehicle insurance\",\n",
    "        \"Vehicle maintenance\",\n",
    "    ],\n",
    "    \"Life and Entertainment\": [\n",
    "        \"Life & Entertainment\",\n",
    "        \"Active sport, fitness\",\n",
    "        \"Alcohol, tobacco\",\n",
    "        \"Books, audio, subscriptions\",\n",
    "        \"Charity, gifts\",\n",
    "        \"Culture, sport events\",\n",
    "        \"Education, development\",\n",
    "        \"Health care, doctor\",\n",
    "        \"Hobbies\",\n",
    "        \"Holiday, trips, hotels\",\n",
    "        \"Sightseeing, activities\",\n",
    "        \"Accommodation\",\n",
    "        \"Life events\",\n",
    "        \"Lottery, gambling\",\n",
    "        \"TV, Streaming\",\n",
    "        \"Wellness, beauty\",\n",
    "    ],\n",
    "    \"Communication and PC\": [\n",
    "        \"Communication, PC\",\n",
    "        \"Internet\",\n",
    "        \"Phone, mobile phone\",\n",
    "        \"Postal services\",\n",
    "        \"Software, apps, games\",\n",
    "        \"Phone, cell phone\",\n",
    "    ],\n",
    "    \"Financial Expenses\": [\n",
    "        \"Financial expenses\",\n",
    "        \"Advisory\",\n",
    "        \"Charges, Fees\",\n",
    "        \"Fines\",\n",
    "        \"Insurances\",\n",
    "        \"Loan, interests\",\n",
    "        \"Taxes\",\n",
    "    ],\n",
    "    \"Investments\": [\n",
    "        \"Investments\",\n",
    "        \"Financial investments\",\n",
    "        \"Collections\",\n",
    "        \"Realty\",\n",
    "        \"Savings\",\n",
    "        \"Vehicles, chattels\",\n",
    "    ],\n",
    "    \"Income\": [\"Income\", \"Gifts\", \"Refunds (tax, purchase)\", \"Sale\", \"Wage, invoices\", \"Lending, renting\", \"Rentals\"],\n",
    "    \"Other\": [\"Missing\", \"Other\"],\n",
    "}\n",
    "\n",
    "d_nat = {\n",
    "    \"need\": [\n",
    "        \"Food & Drinks\",\n",
    "        \"Groceries\",\n",
    "        \"Restaurant, fast-food\",\n",
    "        \"Clothes & shoes\",\n",
    "        \"Drug-store, chemist\",\n",
    "        \"Teeth care\",\n",
    "        \"Supplements\",\n",
    "        \"Medicine\",\n",
    "        \"Home, garden\",\n",
    "        \"Housing\",\n",
    "        \"Energy, utilities\",\n",
    "        \"Maintenance, repairs\",\n",
    "        \"Rent\",\n",
    "        \"Transportation\",\n",
    "        \"Long distance\",\n",
    "        \"Public transport\",\n",
    "        \"Taxi\",\n",
    "        \"Active sport, fitness\",\n",
    "        \"Communication, PC\",\n",
    "        \"Internet\",\n",
    "        \"Phone, mobile phone\",\n",
    "        \"Postal services\",\n",
    "        \"Phone, cell phone\",\n",
    "        \"Charges, Fees\",\n",
    "        \"Fines\",\n",
    "        \"Insurances\",\n",
    "        \"Loan, interests\",\n",
    "        \"Taxes\",\n",
    "        \"Other\",\n",
    "        \"Missing\",\n",
    "        \"Housing\",\n",
    "        'Financial expenses',\n",
    "    ],\n",
    "\n",
    "    'want': [\n",
    "        \"Bar, cafe\",\n",
    "        \"Fitness Supplements\",\n",
    "        \"Coffee\",\n",
    "        \"Eating out\"\n",
    "        \"Shopping\",\n",
    "        \"Electronics, accessories\",\n",
    "        \"Camera expenses\",\n",
    "        \"Free time\",\n",
    "        \"Gifts, joy\",\n",
    "        \"Health and beauty\",\n",
    "        \"Skincare face\",\n",
    "        \"Skincare body\"\n",
    "        \"Jewels, accessories\",\n",
    "        \"Stationery, tools\",\n",
    "        \"Business trips\",\n",
    "        \"Vehicle\",\n",
    "        \"Fuel\",\n",
    "        \"Leasing\",\n",
    "        \"Parking\",\n",
    "        \"Rentals\",\n",
    "        \"Vehicle insurance\",\n",
    "        \"Vehicle maintenance\",\n",
    "        \"Life & Entertainment\",\n",
    "        \"Alcohol, tobacco\",\n",
    "        \"Books, audio, subscriptions\",\n",
    "        \"Charity, gifts\",\n",
    "        \"Culture, sport events\",\n",
    "        \"Education, development\",\n",
    "        \"Health care, doctor\",\n",
    "        \"Hobbies\",\n",
    "        \"Holiday, trips, hotels\",\n",
    "        \"Sightseeing, activities\",\n",
    "        \"Accommodation\",\n",
    "        \"Life events\",\n",
    "        \"Lottery, gambling\",\n",
    "        \"TV, Streaming\",\n",
    "        \"Wellness, beauty\",\n",
    "        \"Software, apps, games\",\n",
    "        \"Advisory\",\n",
    "        'Shopping'\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to flatten dict\n",
    "def flatten_dict(d):\n",
    "    \"\"\"This function flattens dictionaries\"\"\"\n",
    "    nd = {}\n",
    "    for k, v in d.items():\n",
    "        # Check if it's a list, if so then iterate through\n",
    "        if hasattr(v, \"__iter__\") and not isinstance(v, str):\n",
    "            for item in v:\n",
    "                nd[item] = k\n",
    "        else:\n",
    "            nd[v] = k\n",
    "    return nd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the category and nature dictionaries\n",
    "flatten_d = flatten_dict(d)\n",
    "flatten_d_nat = flatten_dict(d_nat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the category to subcategory\n",
    "df = df.rename(columns={'category': 'subcategory'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the values from dictionaries to corresponding values in data frame\n",
    "df[\"category\"] = df[\"subcategory\"].map(flatten_d)\n",
    "df['nature'] = df['subcategory'].map(flatten_d_nat)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the amount variables to absolute values\n",
    "df[[\"amount\", \"ref_currency_amount\"]] = df[[\n",
    "    \"amount\", \"ref_currency_amount\"]].abs()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the date column to date only and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = df['date'].dt.time\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Split the *Labels* column to 3 columns as it contains multiple values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"l1\", \"l2\", \"l3\", \"l4\"]\n",
    "         ] = df[\"labels\"].str.rsplit(\"|\", expand=True)\n",
    "df[[\"l1\", \"l3\", \"l3\", \"l4\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are mixed across these 4 label columns. I convert these Series to lists to bring the values in correct place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the the splitted columns to lists to iterate and change the values\n",
    "list_1 = df[\"l1\"].to_list()\n",
    "list_2 = df[\"l2\"].to_list()\n",
    "list_3 = df[\"l3\"].to_list()\n",
    "list_4 = df[\"l4\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values (these are the place names)\n",
    "places = list(df[\"l3\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with invalid names or NaN values\n",
    "del_place = [1, 2, 6, 18]\n",
    "# remove and using numpy and convert back to list\n",
    "places_1 = np.delete(places, del_place).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through list_3 -- there are the majority of correct values.\n",
    "# Iterate through it and if the value is not in the list with correct places\n",
    "# look in other columns and append to a new list\n",
    "nvalid = (\"BIG TRIP\", \"Thailand\")\n",
    "place = []\n",
    "for x in list_3:\n",
    "    if x in places_1:\n",
    "        place.append(x)\n",
    "    elif x in nvalid and list_2[list_1.index(x)] in nvalid:\n",
    "        place.append(list_1[list_3.index(x)])\n",
    "    elif x in nvalid and list_1[list_3.index(x)] in nvalid:\n",
    "        place.append(list_2[list_1.index(x)])\n",
    "    elif x == \"Accommodation\":\n",
    "        x = list_4[list_3.index(x)]\n",
    "        place.append(x)\n",
    "    else:\n",
    "        place.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the new list to the data frame\n",
    "df[\"place\"] = place\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude/filter out deposit entries (deposits for hotel rooms etc.)\n",
    "df = df[~df.note.str.contains(\"Deposit\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN values in nature column to NA (for income entries)\n",
    "df['nature'] = df['nature'].fillna(value='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column\n",
    "df[['country', 'lat', 'lng']] = 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data before and during travel\n",
    "start_date = pd.datetime(2021, 10, 2)\n",
    "end_date = pd.datetime(2022, 10, 24)\n",
    "\n",
    "home_df = df.loc[df[\"date\"] < start_date]\n",
    "travel_df = df.loc[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
    "\n",
    "non_travel_exp = [\n",
    "    \"Camera Expenses\",\n",
    "    \"Electronics, accessories\",\n",
    "    \"Books, audio, subscriptions\",\n",
    "    \"Education, development\",\n",
    "]\n",
    "\n",
    "trip_label = \"BIG TRIP\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign conditional column \n",
    "home_df['travel_expense'] = np.where(home_df['labels'].str.contains(trip_label), True, False)\n",
    "travel_df['travel_expense'] = np.where(travel_df['subcategory'].isin(non_travel_exp), False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the travel df for missing values in place column\n",
    "travel_df[travel_df['travel_expense'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the datasets again\n",
    "df = pd.concat([home_df, travel_df])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na in place column with ffill method (forward fill)\n",
    "dftravel[\"place\"].fillna(method=\"ffill\", inplace=True)\n",
    "dftravel.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change values that were not correctly filled in previous step\n",
    "dftravel.loc[dftravel[\"place\"] == \"Accommodation\", [\"place\"]] = \"Phuket\"\n",
    "dftravel.loc[dftravel[\"place\"] == \"Road trip\", [\"place\"]] = \"Sangkhlaburi\"\n",
    "dftravel.loc[dftravel[\"place\"] == \"BIG TRIP\", [\"place\"]] = \"Bangkok\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column 'country'\n",
    "dftravel[\"country\"] = \"Thailand\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally drop not needed columns\n",
    "dftravel.drop([\"labels\", \"l1\", \"l2\", \"l3\", \"l4\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary for each column to spot possible issues\n",
    "dftravel.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Get latitude and longitude for the places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else:\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "keys = dftravel[\"place\"].unique()\n",
    "geodata = list()\n",
    "\n",
    "for place in keys:\n",
    "    parms = dict()\n",
    "    parms['address'] = place\n",
    "\n",
    "    if api_key is not False:\n",
    "        parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "        print(json.dumps(js, indent=4))\n",
    "\n",
    "    lat = js['results'][0]['geometry']['location']['lat']\n",
    "    lng = js['results'][0]['geometry']['location']['lng']\n",
    "    geodata.append([lat, lng])\n",
    "    print('lat', lat, 'lng', lng)\n",
    "    location = js['results'][0]['formatted_address']\n",
    "    print(location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the zip function to make a dict from two lists\n",
    "geo_dict = dict(zip(keys, geodata))\n",
    "geo_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally map the dict values to the dataframe\n",
    "dftravel[\"gdata\"] = dftravel[\"place\"].map(geo_dict)\n",
    "dftravel.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitude and longitude are stored in one column, I split the column to two columns\n",
    "dftravel[[\"lat\", \"lng\"]] = pd.DataFrame(\n",
    "    dftravel.gdata.to_list(), index=dftravel.index)\n",
    "dftravel.drop(\"gdata\", axis=1, inplace=True)\n",
    "print(dftravel.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the column order\n",
    "col_names = dftravel.columns.values.tolist()\n",
    "col_order = ['date',\n",
    "             'year',\n",
    "             'month',\n",
    "             'day',\n",
    "             'weekday',\n",
    "             'time',\n",
    "             'category',\n",
    "             'subcategory',\n",
    "             'nature',\n",
    "             'amount',\n",
    "             'account',\n",
    "             'payment_type',\n",
    "             'lat',\n",
    "             'lng',\n",
    "             'place',\n",
    "             'country']\n",
    "\n",
    "dftravel = dftravel.reindex(columns=col_order)\n",
    "dftravel.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Write the data to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftravel.to_csv(\"data/2022-09-27_travel_expenses.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Write the data to a SQLite database file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to a SQLite database file\n",
    "import sqlite3 as sq\n",
    "\n",
    "sql_data = \"data/EXPENSES.db\"\n",
    "conn = sq.connect(sql_data)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"DROP TABLE IF EXISTS travel_expenses\"\"\")\n",
    "dftravel.to_sql(\"dftravel\", conn, if_exists=\"replace\", index=False)\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b4d75ac280b6c7c3aa43866cb82dc88915409b55fec83a093dd0284cb58708e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
