{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Personal Expenses Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Loading and first look**  \n",
    "Export data from the smartphone app I use to collect my expense data. The data comes in a handy CSV format, so I can easily load it into a pandas DataFrame by specifying a delimiter. I also specify other parameters: which columns to load and parsing dates from the 'date' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# save file path to a variable\n",
    "fname = \"data/report_2022-08-05_110949.csv\"\n",
    "# load the data\n",
    "df = pd.read_csv(\n",
    "    fname,\n",
    "    sep=\";\",\n",
    "    usecols=[\n",
    "        \"date\",\n",
    "        \"category\",\n",
    "        \"account\",\n",
    "        \"ref_currency_amount\",\n",
    "        \"payment_type_local\",\n",
    "        \"gps_latitude\",\n",
    "        \"gps_longitude\",\n",
    "        \"labels\",\n",
    "    ],\n",
    "    parse_dates=[\"date\"],\n",
    ")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly adjust the column names to something more meaningful to me and change the order.\n",
    "# change the 'category' to 'subcategory' the values actually refer to subcategories. I will add 'category' column later.\n",
    "df.columns = [\n",
    "    \"date\",\n",
    "    \"account\",\n",
    "    \"subcategory\",\n",
    "    \"amount\",\n",
    "    \"payment_type\",\n",
    "    \"lat\",\n",
    "    \"long\",\n",
    "    \"labels\",\n",
    "]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Cleaning and Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Handling Missing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary of each column\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I see that the 'lat' and 'long' geodata columns do not contain any values. So far this is fine, I will get the place names from the 'labels' column and use the geopy library to get the relevant data about the places I have visited during my travels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and if necessary remove duplicates\n",
    "df.duplicated()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I add the category names. The exported data set doesn't contain this data, so I copied it manually from the application and created a dictionary(***category : subcategory***)\n",
    "After that, I will map the category value to each row based on the subcategory using pandas **map()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with categories as keys and subcategories as values\n",
    "# also assign the missing category for Fitness Supplements\n",
    "\n",
    "d = {\n",
    "    \"Food_drinks\": [\n",
    "        \"Food & Drinks\",\n",
    "        \"Bar, cafe\",\n",
    "        \"Groceries\",\n",
    "        \"Restaurant, fast-food\",\n",
    "        \"Fitness Supplements\",\n",
    "    ],\n",
    "    \"Shopping\": [\n",
    "        \"Shopping\",\n",
    "        \"Clothes & shoes\",\n",
    "        \"Drug-store, chemist\",\n",
    "        \"Electronics, accessories\",\n",
    "        \"Camera expenses\",\n",
    "        \"Free time\",\n",
    "        \"Gifts, joy\",\n",
    "        \"Health and beauty\",\n",
    "        \"Home, garden\",\n",
    "        \"Jewels, accessories\",\n",
    "        \"Stationery, tools\",\n",
    "    ],\n",
    "    \"Housing\": [\"Housing\", \"Energy, utilities\", \"Maintenance, repairs\", \"Rent\"],\n",
    "    \"Transportation\": [\n",
    "        \"Transportation\",\n",
    "        \"Business trips\",\n",
    "        \"Long distance\",\n",
    "        \"Public transport\",\n",
    "        \"Taxi\",\n",
    "    ],\n",
    "    \"Vehicle\": [\n",
    "        \"Vehicle\",\n",
    "        \"Fuel\",\n",
    "        \"Leasing\",\n",
    "        \"Parking\",\n",
    "        \"Rentals\",\n",
    "        \"Vehicle insurance\",\n",
    "        \"Vehicle maintenance\",\n",
    "    ],\n",
    "    \"Life_Entertainment\": [\n",
    "        \"Life & Entertainment\",\n",
    "        \"Active sport, fitness\",\n",
    "        \"Alcohol, tobacco\",\n",
    "        \"Books, audio, subscriptions\",\n",
    "        \"Charity, gifts\",\n",
    "        \"Culture, sport events\",\n",
    "        \"Education, development\",\n",
    "        \"Health care, doctor\",\n",
    "        \"Hobbies\",\n",
    "        \"Holiday, trips, hotels\",\n",
    "        \"Sightseeing, activities\",\n",
    "        \"Accommodation\",\n",
    "        \"Life events\",\n",
    "        \"Lottery, gambling\",\n",
    "        \"TV, Streaming\",\n",
    "        \"Wellness, beauty\",\n",
    "    ],\n",
    "    \"Communication_PC\": [\n",
    "        \"Communication, PC\",\n",
    "        \"Internet\",\n",
    "        \"Phone, mobile phone\",\n",
    "        \"Postal services\",\n",
    "        \"Software, apps, games\",\n",
    "        \"Phone, cell phone\",\n",
    "    ],\n",
    "    \"Financial_expenses\": [\n",
    "        \"Financial expenses\",\n",
    "        \"Advisory\",\n",
    "        \"Charges, Fees\",\n",
    "        \"Fines\",\n",
    "        \"Insurances\",\n",
    "        \"Loan, interests\",\n",
    "        \"Taxes\",\n",
    "    ],\n",
    "    \"Investments\": [\n",
    "        \"Investments\",\n",
    "        \"Financial investments\",\n",
    "        \"Collections\",\n",
    "        \"Realty\",\n",
    "        \"Savings\",\n",
    "        \"Vehicles, chattels\",\n",
    "    ],\n",
    "    \"Income\": [\"Income\", \"Gifts\", \"Refunds (tax, purchase)\", \"Sale\", \"Wage, invoices\"],\n",
    "    \"Other\": [\"Missing\", \"Other\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dictionary needs to be flatten before using the map function\n",
    "def flatten_dict(d):\n",
    "    nd = {}\n",
    "    for k, v in d.items():\n",
    "        # Check if it's a list, if so then iterate through\n",
    "        if hasattr(v, \"__iter__\") and not isinstance(v, str):\n",
    "            for item in v:\n",
    "                nd[item] = k\n",
    "        else:\n",
    "            nd[v] = k\n",
    "    return nd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the new function to flatten the dict\n",
    "flatten_d = flatten_dict(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally map using the pandas map() function to assign the values\n",
    "df[\"category\"] = df[\"subcategory\"].map(flatten_d)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the column order\n",
    "df = df[\n",
    "    [\n",
    "        \"date\",\n",
    "        \"category\",\n",
    "        \"subcategory\",\n",
    "        \"amount\",\n",
    "        \"account\",\n",
    "        \"payment_type\",\n",
    "        \"lat\",\n",
    "        \"long\",\n",
    "        \"labels\",\n",
    "    ]\n",
    "]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the amount column to absolute value\n",
    "df[\"amount\"] = df[\"amount\"].abs()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Create a subset DataFrame for a time period at home.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the DataFrame to 2 DataFrames\n",
    "dfhome = df.loc[\n",
    "    (df[\"date\"] < \"2021-10-02T00:00:00\"),\n",
    "    [\"date\", \"category\", \"subcategory\", \"amount\", \"account\", \"payment_type\"],\n",
    "].reset_index(drop=True)\n",
    "dfhome.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Create a subset of the DataFrame containing expenses while travelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df subset with data while travelling\n",
    "cols = list(df.columns)\n",
    "dftravel = df.loc[df[\"date\"] > \"2021-10-02T00:00:00\", cols].reset_index(drop=True)\n",
    "dftravel.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the deposit records as they don't count as expenses\n",
    "# filter out the records\n",
    "dftravel = dftravel[\n",
    "    ~(\n",
    "        (dftravel[\"category\"] == \"Financial_expenses\")\n",
    "        & (dftravel[\"subcategory\"] == \"Loan, interests\")\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Split the *Labels* column to 3 columns as it contains multiple values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftravel[[\"l1\", \"l2\", \"l3\", \"l4\"]] = dftravel[\"labels\"].str.rsplit(\"|\", expand=True)\n",
    "dftravel[[\"l1\", \"l3\", \"l3\", \"l4\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are mixed across these 4 label columns. I convert these Series to lists to bring the values in correct place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the the splitted columns to lists to iterate and change the values\n",
    "list_1 = dftravel[\"l1\"].to_list()\n",
    "list_2 = dftravel[\"l2\"].to_list()\n",
    "list_3 = dftravel[\"l3\"].to_list()\n",
    "list_4 = dftravel[\"l4\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "places = list(dftravel[\"l3\"].unique())  # get unique values (These are the place names)\n",
    "del_place = [1, 4, 6, 7, 19] # create a list with invalid names or NaN values\n",
    "places_1 = np.delete(places, del_place).tolist() # remove and using numpy and convert back to list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through list_3 -- there are the majority of correct values.\n",
    "# Iterate through it and if the value is not in the list with correct places\n",
    "# look in other columns and append to a new list\n",
    "nvalid = (\"BIG TRIP\", \"Thailand\")\n",
    "place = []\n",
    "for x in list_3:\n",
    "    if x in places_1:\n",
    "        place.append(x)\n",
    "    elif x in nvalid and list_2[list_1.index(x)] in nvalid:\n",
    "        place.append(list_1[list_3.index(x)])\n",
    "    elif x in nvalid and list_1[list_3.index(x)] in nvalid:\n",
    "        place.append(list_2[list_1.index(x)])\n",
    "    elif x == \"Accomodation\":\n",
    "        x = list_4[list_1.index(x)]\n",
    "        place.append(x)\n",
    "    else:\n",
    "        place.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the new list to the dataframe\n",
    "dftravel[\"place\"] = place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na in place column with ffill method (forward fill)\n",
    "dftravel[\"place\"].fillna(method=\"ffill\", inplace=True)\n",
    "dftravel.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change values that were not correctly filled in previous step\n",
    "dftravel.loc[dftravel[\"place\"] == \"Accommodation\", [\"place\"]] = \"Phuket\"\n",
    "dftravel.loc[dftravel[\"place\"] == \"Road trip\", [\"place\"]] = \"Sangkhlaburi\"\n",
    "dftravel.loc[dftravel[\"place\"] == \"BIG TRIP\", [\"place\"]] = \"Bangkok\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column 'country'\n",
    "dftravel[\"country\"] = \"Thailand\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally drop not needed columns\n",
    "dftravel.drop([\"labels\", \"l1\", \"l2\", \"l3\", \"l4\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary for each column to spot possible issues\n",
    "dftravel.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Get latitude and longitude for the places using the geopy library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup the geodata for each place from the list and store it in another list\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"disbalanxx@gmail.com\")\n",
    "keys = dftravel[\"place\"].unique()\n",
    "geodata = []\n",
    "for x in keys:\n",
    "    location = geolocator.geocode(x)\n",
    "    print(f\"Fetching geodata of {x} and appending to the list\")\n",
    "    geodata.append(str(location.latitude) + \" \" + str(location.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the zip function to make a dict from two lists\n",
    "geo_dict = dict(zip(keys, geodata))\n",
    "geo_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally map the dict values to the dataframe\n",
    "dftravel[\"gdata\"] = dftravel[\"place\"].map(geo_dict)\n",
    "dftravel.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitude and longitude are stored in one column, I split the column to two columns\n",
    "dftravel[[\"lat\", \"long\"]] = dftravel[\"gdata\"].str.rsplit(expand=True)\n",
    "dftravel.drop(\"gdata\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the geodata to float number type\n",
    "convert = {\"lat\": float, \"long\": float}\n",
    "dftravel = dftravel.astype(convert)\n",
    "print(dftravel.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Write the data to a SQLite database file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write travel expenses dataframe to a SQLite data base file\n",
    "import sqlite3 as sq\n",
    "\n",
    "data = dftravel\n",
    "sql_data = \"EXPENSES.db\"\n",
    "conn = sq.connect(sql_data)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"DROP TABLE IF EXISTS travel_expenses\"\"\")\n",
    "data.to_sql(\n",
    "    \"travel_expenses\", conn, if_exists=\"replace\", index=False\n",
    ")  # - writes the pd.df to SQLIte DB\n",
    "pd.read_sql(\"select * from travel_expenses\", conn)\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write home expense dataframe to a SQLite data base file\n",
    "\n",
    "data = dfhome\n",
    "sql_data = \"EXPENSES\"  # - Creates DB names SQLite\n",
    "conn = sq.connect(sql_data)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"DROP TABLE IF EXISTS home_expenses\"\"\")\n",
    "data.to_sql(\n",
    "    \"home_expenses\", conn, if_exists=\"replace\", index=False\n",
    ")  # - writes the pd.df to SQLIte DB\n",
    "pd.read_sql(\"select * from home_expenses\", conn)\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2047dafdc30c066b6977978002be79ed5df509c6ea5cf7f750aba6e19bb0e9d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
